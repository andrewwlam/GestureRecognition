"""APS360 Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Rt_xHGujYREZ4CC-HyhJzNeYQfYWhaa

# **APS360 Project**
#**Live Gesture Recognition Model for Emoji Choice**

Andrew Lam, Dan Kim, Marie Floryan

# **Section 1: Retrieving Dataset**

LINK TO LIVE FEED APPLICATION CODE: https://github.com/donghee214/gesture-recognition

**Data Processing**


Each group member will get 105 samples of each emoji, for a total of 315 samples total of each emoji. 


Of the 105 samples from each person:
35 pictures will be taken in dark, neutral, and bright lighting.

White background will be used.
Pictures resized to 224x224.
Hand is centered in the center of the image.

Data labelling:
“Image number”_”lighting”_”emoji name”




**1.1 Data Loading**
"""

#Andrew

#mount googledrive
from google.colab import drive
drive.mount('/content/gdrive')

# Andrew


#Loading Gesture Images from Google Drive

import torch
import numpy as np

import torchvision
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import matplotlib.pyplot as plt

# location on Google Drive
train_path = '/content/gdrive/My Drive/hand_gesture_dataset/train'
valid_path = '/content/gdrive/My Drive/hand_gesture_dataset/valid'
test_path = '/content/gdrive/My Drive/hand_gesture_dataset/test'
overfit_path = '/content/gdrive/My Drive/hand_gesture_dataset/overfit'

# Transform Settings - Do not use RandomResizedCrop
transform = transforms.Compose([transforms.Resize((224,224)), 
                                transforms.ToTensor()])

# Load data from Google Drive
trainset = torchvision.datasets.ImageFolder(train_path, transform=transform)
valset = torchvision.datasets.ImageFolder(valid_path, transform=transform)
testset = torchvision.datasets.ImageFolder(test_path, transform=transform)

# Prepare Dataloader
data_size = len(trainset)
batch_size = 32
num_workers = 1
data_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, 
                                           num_workers=num_workers, shuffle=True)

# Verification Step - obtain one batch of images
dataiter = iter(data_loader)
images, labels = dataiter.next()
images = images.numpy() # convert images to numpy for display

classes = ['fi', 'no', 'ok', 'pt', 'ro', 'tu', 'up']

# plot the images in the batch, along with the corresponding labels
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    plt.imshow(np.transpose(images[idx], (1, 2, 0)))
    ax.set_title(classes[labels[idx]])

"""**Splitting Dataset**"""

# 315 images for each emoji

# 40/40/20 split? 

# so 126/126/63 for each emoji  #126 isnt alot for the training... maybe could do a 50/30/20 split? but dunno if this will have a big effect or not

# 882/882/441
batch_size = 64
num_workers = 1
train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, shuffle=True)
val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, num_workers=num_workers, shuffle=True)
test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=num_workers, shuffle=True)

###### BASELINE MODEL #######

#https://www.geeksforgeeks.org/python-image-classification-using-keras/

# importing libraries 
from keras.preprocessing.image import ImageDataGenerator 
from keras.models import Sequential 
from keras.layers import Conv2D, MaxPooling2D 
from keras.layers import Activation, Dropout, Flatten, Dense 
from keras import backend as K 
  
  
img_width, img_height = 224, 224
  
train_data_dir = train_path
validation_data_dir = valid_path
nb_train_samples = 875 
nb_validation_samples = 875
epochs = 10
batch_size = 16
  
if K.image_data_format() == 'channels_first': 
    input_shape = (3, img_width, img_height) 
else: 
    input_shape = (img_width, img_height, 3) 
  
model = Sequential() 
model.add(Conv2D(32, (2, 2), input_shape = input_shape)) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size =(2, 2))) 
  
model.add(Conv2D(32, (2, 2))) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size =(2, 2))) 
  
model.add(Conv2D(64, (2, 2))) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size =(2, 2))) 
  
model.add(Flatten()) 
model.add(Dense(64)) 
model.add(Activation('relu')) 
model.add(Dropout(0.5)) 
model.add(Dense(7)) 
model.add(Activation('softmax')) 
  
model.compile(loss ='sparse_categorical_crossentropy', 
                     optimizer ='rmsprop', 
                   metrics =['accuracy']) 
  
train_datagen = ImageDataGenerator( 
                rescale = 1. / 255, 
                 shear_range = 0.2, 
                  zoom_range = 0.2, 
            horizontal_flip = True) 
  
test_datagen = ImageDataGenerator(rescale = 1. / 255) 
  
train_generator = train_datagen.flow_from_directory(train_data_dir, 
                              target_size =(img_width, img_height), 
                     batch_size = batch_size, class_mode ='sparse') 
  
validation_generator = test_datagen.flow_from_directory( 
                                    validation_data_dir, 
                   target_size =(img_width, img_height), 
          batch_size = batch_size, class_mode ='sparse') 
  
model.fit_generator(train_generator, 
    steps_per_epoch = nb_train_samples // batch_size, 
    epochs = epochs, validation_data = validation_generator, 
    validation_steps = nb_validation_samples // batch_size)
