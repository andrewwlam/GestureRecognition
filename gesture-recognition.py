# -*- coding: utf-8 -*-
"""APS360 Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Rt_xHGujYREZ4CC-HyhJzNeYQfYWhaa

# **APS360 Project**
#**Live Gesture Recognition Model for Emoji Choice**

Andrew Lam, Dan Kim, Marie Floryan

# **Section 1: Retrieving Dataset**

LINK TO LIVE FEED APPLICATION CODE: https://github.com/donghee214/gesture-recognition

**Data Processing**


Each group member will get 105 samples of each emoji, for a total of 315 samples total of each emoji. 


Of the 105 samples from each person:
35 pictures will be taken in dark, neutral, and bright lighting.

White background will be used.
Pictures resized to 224x224.
Hand is centered in the center of the image.

Data labelling:
“Image number”_”lighting”_”emoji name”




**1.1 Data Loading**
"""

#Andrew

#mount googledrive
from google.colab import drive
drive.mount('/content/gdrive')

# Andrew


#Loading Gesture Images from Google Drive

import torch
import numpy as np

import torchvision
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import matplotlib.pyplot as plt

# location on Google Drive
train_path = '/content/gdrive/My Drive/hand_gesture_dataset/train'
valid_path = '/content/gdrive/My Drive/hand_gesture_dataset/valid'
test_path = '/content/gdrive/My Drive/hand_gesture_dataset/test'
overfit_path = '/content/gdrive/My Drive/hand_gesture_dataset/overfit'

# Transform Settings - Do not use RandomResizedCrop
transform = transforms.Compose([transforms.Resize((224,224)), 
                                transforms.ToTensor()])

# Load data from Google Drive
trainset = torchvision.datasets.ImageFolder(train_path, transform=transform)
valset = torchvision.datasets.ImageFolder(valid_path, transform=transform)
testset = torchvision.datasets.ImageFolder(test_path, transform=transform)

# Prepare Dataloader
data_size = len(trainset)
batch_size = 32
num_workers = 1
data_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, 
                                           num_workers=num_workers, shuffle=True)

# Verification Step - obtain one batch of images
dataiter = iter(data_loader)
images, labels = dataiter.next()
images = images.numpy() # convert images to numpy for display

classes = ['fi', 'no', 'ok', 'pt', 'ro', 'tu', 'up']

# plot the images in the batch, along with the corresponding labels
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    plt.imshow(np.transpose(images[idx], (1, 2, 0)))
    ax.set_title(classes[labels[idx]])

"""**Splitting Dataset**"""

# 315 images for each emoji

# 40/40/20 split? 

# so 126/126/63 for each emoji  #126 isnt alot for the training... maybe could do a 50/30/20 split? but dunno if this will have a big effect or not

# 882/882/441
batch_size = 64
num_workers = 1
train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, shuffle=True)
val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, num_workers=num_workers, shuffle=True)
test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=num_workers, shuffle=True)

""" **Section 2: Building Model**

"""
#Andrew

import torch.nn as nn
import torch.nn.functional as F

import matplotlib.pyplot as plt # for plotting
import torch.optim as optim #for gradient descent
import time
torch.manual_seed(1) # set the random seed



class Emoji_Classifier(nn.Module):
      def __init__(self):
          super(Emoji_Classifier, self).__init__()
          self.name = "net"
          self.conv1 = nn.Conv2d(3, 10, 5, 2) #in_channels, out_chanels, kernel_size
          self.pool = nn.MaxPool2d(2, 2) #kernel_size, stride 
          self.conv2 = nn.Conv2d(10, 26, 5, 2) #in_channels, out_chanels, kernel_size
          self.fc1 = nn.Linear(26 * 13 * 13, 120)
          self.fc2 = nn.Linear(120, 8)

      def forward(self, x):
          x = self.pool(F.relu(self.conv1(x)))
          x = self.pool(F.relu(self.conv2(x)))
          x = x.view(-1, 26 * 13 * 13)
          x = F.relu(self.fc1(x))
          x = self.fc2(x)
          x = x.squeeze(1) #Flatten to batch size
          return x

"""# **Section 3: Training & Hyperparameters**

**3.1 Training**
"""

#Marie
def evaluate(net, loader, criterion):
    """ Evaluate the network on the validation set.

     Args:
         net: PyTorch neural network object
         loader: PyTorch data loader for the validation set
         criterion: The loss function
     Returns:
         err: A scalar for the avg classification error over the validation set
         loss: A scalar for the average loss function over the validation set
     """
    total_loss = 0.0
    total_err = 0.0
    total_epoch = 0
    for i, data in enumerate(loader, 0):
       # if use_cuda and torch.cuda.is_available():
        #      data = data.cuda()
        inputs, labels = data
        if use_cuda and torch.cuda.is_available():
              labels = labels.cuda()
              inputs = inputs.cuda()
        labels = normalize_label(labels)  # Convert labels to 0/1
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        corr = (outputs > 0.0).squeeze().long() != labels
        total_err += int(corr.sum())
        total_loss += loss.item()
        total_epoch += len(labels)
    err = float(total_err) / total_epoch
    loss = float(total_loss) / (i + 1)
    return err, loss

def normalize_label(labels):
    """
    Given a tensor containing 2 possible values, normalize this to 0/1

    Args:
        labels: a 1D tensor containing two possible scalar values
    Returns:
        A tensor normalize to 0/1 value
    """
    max_val = torch.max(labels)
    min_val = torch.min(labels)
    norm_labels = (labels - min_val)/(max_val - min_val)
    return norm_labels

#Marie
def train(model, use_cuda, train_loader, val_loader, batch_size=27, num_epochs=1, learn_rate = 0.001):

    torch.manual_seed(1000)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learn_rate)

    iters, losses, train_acc, val_acc, val_loss = [], [], [], [], []

    # training
    print ("Training Started...")
    n = 0 # the number of iterations
    for epoch in range(num_epochs):
        for imgs, labels in iter(train_loader):
            if use_cuda and torch.cuda.is_available():
              imgs = imgs.cuda()
              labels = labels.cuda()

            out = model(imgs)             # forward pass
            loss = criterion(out, labels) # compute the total loss
            loss.backward()               # backward pass (compute parameter updates)
            optimizer.step()              # make the updates for each parameter
            optimizer.zero_grad()         # a clean up step for PyTorch
            n += 1    
        # track accuracy
        iters.append(n)
        losses.append(float(loss)/batch_size)
        train_acc.append(get_accuracy(model, train_loader, use_cuda))
        val_acc.append(get_accuracy(model, val_loader, use_cuda))
        #val_loss.append(evaluate(model, val_loader, criterion))
        print(("Epoch {}: Train acc: {}, Train loss: {} |"+
               "Validation acc: {}").format(
                   epoch + 1,
                   train_acc[-1],
                   losses[-1],
                   val_acc[-1]))

        # plotting
    plt.title("Training Curve")
    plt.plot(iters, losses, label="Train")
    #plt.plot(iters, val_loss, label="Validation")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    #plt.legend(loc='best')
    plt.show()

    plt.title("Training Curve")
    plt.plot(iters, train_acc, label="Train")
    plt.plot(iters, val_acc, label="Validation")
    plt.xlabel("Iterations")
    plt.ylabel("Training Accuracy")
    plt.legend(loc='best')
    plt.show()

    print("Final Training Accuracy: {}".format(train_acc[-1]))
    print("Final Validation Accuracy: {}".format(val_acc[-1]))     
    return train_acc, val_acc

#Marie
def get_accuracy(model, data_loader, use_cuda):
    correct = 0
    total = 0

    for imgs, labels in data_loader:
        
        if use_cuda and torch.cuda.is_available():
          imgs = imgs.cuda()
          labels = labels.cuda()

        output = model(imgs)
        #select index with maximum prediction score
        pred = output.max(1, keepdim=True)[1]
        correct += pred.eq(labels.view_as(pred)).sum().item()
        total += imgs.shape[0]
    return correct / total

model = Emoji_Classifier()
model.cuda()
use_cuda = True
# use_cuda set to false
train(model, True, train_loader, val_loader, batch_size= 32, num_epochs= 10, learn_rate=0.001)

#save & export the model 
torch.save(model.state_dict(), "/content/gdrive/My Drive/trained_model/model_dict")
torch.save(model, "/content/gdrive/My Drive/trained_model/model")



get_accuracy(model, val_loader, use_cuda)

"""**3.2 “Overfit” to a Small Dataset**

Overfitting is one way to sanity check the model. We will construct a small dataset (e.g. just the images that you have collected), then show that our model and
training code is capable of memorizing the labels of this small data set.

With a large batch size (e.g. the entire small dataset) and learning rate that is not too high, we should be
able to obtain a 100% training accuracy on that small dataset relatively quickly (within 200 iterations).
"""

#Andrew
def overtrain(model, use_cuda, train_loader, batch_size=27, num_epochs=1, learn_rate = 0.001):

    torch.manual_seed(1000)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learn_rate)

    iters, losses, train_acc, val_acc, val_loss = [], [], [], [], []

    # training
    print ("Training Started...")
    n = 0 # the number of iterations
    for epoch in range(num_epochs):
        for imgs, labels in iter(train_loader):
            if use_cuda and torch.cuda.is_available():
              imgs = imgs.cuda()
              labels = labels.cuda()

            out = model(imgs)             # forward pass
            loss = criterion(out, labels) # compute the total loss
            loss.backward()               # backward pass (compute parameter updates)
            optimizer.step()              # make the updates for each parameter
            optimizer.zero_grad()         # a clean up step for PyTorch
            n += 1    
        # track accuracy
        iters.append(n)
        losses.append(float(loss)/batch_size)
        train_acc.append(get_accuracy(model, train_loader, use_cuda))
        #val_acc.append(get_accuracy(model, val_loader, use_cuda))
        #val_loss.append(evaluate(model, val_loader, criterion))
        print(("Epoch {}: Train acc: {}, Train loss: {} |").format(epoch + 1,train_acc[-1], losses[-1]))

        # plotting
    
    plt.title("Overfitting Training Loss Curve")
    plt.plot(iters, losses, label="Train")
    #plt.plot(iters, val_loss, label="Validation")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    #plt.legend(loc='best')
    plt.show()

    plt.title("Overfitting Training Accuracy Curve")
    plt.plot(iters, train_acc, label="Train")
    #plt.plot(iters, val_acc, label="Validation")
    plt.xlabel("Iterations")
    plt.ylabel("Training Accuracy")
    plt.legend(loc='best')
    plt.show()

    print("Final Training Accuracy: {}".format(train_acc[-1]))
    #print("Final Validation Accuracy: {}".format(val_acc[-1]))     
    return train_acc,

#Andrew
over_dataset = torchvision.datasets.ImageFolder(overfit_path, transform=transform) #will need to make an "Overfit" image folder

# Prepare Dataloader
batch_size = 27
num_workers = 1

over_loader = torch.utils.data.DataLoader(over_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)
model = Emoji_Classifier()
model.cuda()

overtrain(model, True, over_loader, batch_size=27, num_epochs = 50, learn_rate=0.0025)

"""**3.3 Hyperparameter Search**

Running the model with different hyperparameters to determine the optimal configuration for our training model
"""

model = Emoji_Classifier()
model.cuda()
use_cuda = True
train(model, use_cuda, train_loader, val_loader, batch_size= 64, num_epochs= 15, learn_rate=0.001)

model = Emoji_Classifier()
model.cuda()
use_cuda = True
train(model, use_cuda, train_loader, val_loader, batch_size= 64, num_epochs= 15, learn_rate=0.0025)

model = Emoji_Classifier()
model.cuda()
use_cuda = True
train(model, use_cuda, train_loader, val_loader, batch_size= 64, num_epochs= 15, learn_rate=0.003)

model = Emoji_Classifier()
model.cuda()
use_cuda = True
train(model, use_cuda, train_loader, val_loader, batch_size= 64, num_epochs= 10, learn_rate=0.004)

get_accuracy(model, val_loader, use_cuda)

model = Emoji_Classifier()
model.cuda()
use_cuda = True
train(model, use_cuda, train_loader, val_loader, batch_size= 64, num_epochs= 15, learn_rate=0.005)

model = Emoji_Classifier()
model.cuda()
use_cuda = True
train(model, use_cuda, train_loader, val_loader, batch_size= 32, num_epochs= 10, learn_rate=0.004)

model = Emoji_Classifier()
model.cuda()
use_cuda = True
train(model, use_cuda, train_loader, val_loader, batch_size= 64, num_epochs= 10, learn_rate=0.004)

evaluate(model, test_loader, )

evaluate(model, test_loader, use_cuda)

